{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL CSV to Parquet File Conversion\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.csv(\"Downloads/Image_Hash_Embed.csv\",header=False,sep=\",\");\n",
    "starttime = datetime.now()\n",
    "print(\"Start time : \"+str(starttime))\n",
    "df.write.partitionBy(\"_c3\").parquet(\"Desktop/ImageHashEmbeds.parquet\")\n",
    "endtime = starttime - datetime.now()\n",
    "endtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the top-k data from the hash partitioned parquet file.\n",
    "hashed_file = spark.read.parquet(\"Desktop/ImageHashEmbeds.parquet\")\n",
    "hashed_file.createOrReplaceTempView(\"parquetFile\")\n",
    "hashcode = [0, 0, 0, 1]\n",
    "k_value = \"15\"\n",
    "all_value = spark.sql(\"SELECT * FROM parquetFile\")\n",
    "print(\"This is all the values : \\n\")\n",
    "all_value_count = spark.sql(\"SELECT COUNT(*) FROM parquetFile\")\n",
    "print(\"Lenght of parquet : \")\n",
    "all_value_count.show()\n",
    "all_value.show()\n",
    "hashcodes = spark.sql(\"SELECT DISTINCT _c3 FROM parquetFile\")\n",
    "print(\"Distinct hash values : \\n\")\n",
    "hashcodes.show()\n",
    "sql_query = \"SELECT * FROM parquetFile WHERE _c3 ='\" + str(hashcode) + \"' LIMIT \" + k_value\n",
    "hashed = spark.sql(\"SELECT * FROM parquetFile WHERE _c3 ='\" + str(hashcode) + \"' LIMIT \" + k_value)\n",
    "count = spark.sql(\"SELECT COUNT(*) FROM parquetFile WHERE _c3 ='\" + str(hashcode) + \"' LIMIT \" + k_value)\n",
    "\n",
    "hashed.show()\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter \n",
    "from itertools import chain\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn.metrics import recall_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "recall_value_list = []\n",
    "new_list = []\n",
    "for item in [x[\"_c2\"] for x in all_value.rdd.collect()]:\n",
    "    item = item.split(',')\n",
    "    c=0\n",
    "    for i in item:\n",
    "        i = float(i)\n",
    "        item[c] = i\n",
    "        c=c+1\n",
    "    new_list.append(item)\n",
    "new_np_array = np.array(new_list)\n",
    "print(\"np array of embeddings with all the values in parquet file : \\n{}\\n\".format(new_np_array))\n",
    "n=10\n",
    "K = 25\n",
    "complete_bucket_time = []\n",
    "hash_bucket_time = []\n",
    "index = np.random.choice(new_np_array.shape[0], n, replace=False)\n",
    "key_np_array = new_np_array[index]\n",
    "print(\"Key np array : {}\\n\".format(key_np_array))\n",
    "\n",
    "for arr_items in key_np_array:\n",
    "    one_key_list = []\n",
    "    one_key_list.append(arr_items)    \n",
    "    needle_time_complete = []\n",
    "    for cs in range(1):\n",
    "        start_time = time.time()\n",
    "        similarities = cosine_similarity(one_key_list, new_np_array)\n",
    "        needle_time_complete.append((time.time() - start_time))\n",
    "    complete_bucket_time.append((sum(needle_time_complete) / len(needle_time_complete)))\n",
    "    \n",
    "    sim_list = []\n",
    "    for j in similarities:\n",
    "        for k in j:\n",
    "            p = []\n",
    "            p.append(k)\n",
    "            sim_list.append(p)\n",
    "    sim_np_array = np.array(sim_list)\n",
    "    output_array = sorted(list(chain.from_iterable(sim_np_array)), reverse=True)[0:K]\n",
    "    \n",
    "    Dict = {}\n",
    "    for key, value in zip(new_np_array, sim_np_array):\n",
    "        key_list = key.tolist()\n",
    "        Dict[str(key_list)] = value\n",
    "    sorted_d = dict( sorted(Dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sorted_d\n",
    "    top_k_id_list = []\n",
    "    print(\"Sorted Dictionary Size : {}\".format(len(sorted_d)))\n",
    "    sorted_d_n_items = dict(itertools.islice(sorted_d.items(), K))\n",
    "    print(\"Top K Dictionary Size : {}\".format(len(sorted_d_n_items)))\n",
    "    for key in sorted_d_n_items.keys():\n",
    "        key = key[1:-1]\n",
    "        key = key.replace(\" \", \"\")\n",
    "        id_id = spark.sql('SELECT _c0 FROM parquetFile WHERE _c2 =\"' + str(key) + '\"')\n",
    "        for item in [x[\"_c0\"] for x in id_id.rdd.collect()]:\n",
    "            top_k_id_list.append(item)\n",
    "    print(\"Top {} Image ID : {}\".format(K, top_k_id_list))\n",
    "    top_k_id_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    id_id_id = spark.sql('SELECT _c3 FROM parquetFile WHERE _c2 =\"' + key + '\"')\n",
    "    for item in [x[\"_c3\"] for x in id_id_id.rdd.collect()]:\n",
    "        id_id_id_str = item\n",
    "    id_id_id_hash = spark.sql('SELECT * FROM parquetFile WHERE _c3 =\"' + str(id_id_id_str) + '\"')\n",
    "\n",
    "\n",
    "    new_list = []\n",
    "    for item in [x[\"_c2\"] for x in id_id_id_hash.rdd.collect()]:\n",
    "        item = item.split(',')\n",
    "        c=0\n",
    "        for i in item:\n",
    "            i = float(i)\n",
    "            item[c] = i\n",
    "            c=c+1\n",
    "        new_list.append(item)\n",
    "    new_np_array = np.array(new_list)\n",
    "    print(\"np array of embeddings for hash value {} in parquet file : \\n{}\\n\".format(id_id_id_str, new_np_array))\n",
    "\n",
    "    needle_time_bucket = []\n",
    "    for cs in range(1):\n",
    "        start_time_bucket = time.time()\n",
    "        similarities = cosine_similarity(one_key_list, new_np_array)\n",
    "        needle_time_bucket.append((time.time() - start_time_bucket))\n",
    "    hash_bucket_time.append((sum(needle_time_bucket) / len(needle_time_bucket)))\n",
    "    sim_list = []\n",
    "    for j in similarities:\n",
    "        for k in j:\n",
    "            p = []\n",
    "            p.append(k)\n",
    "            sim_list.append(p)\n",
    "    sim_np_array = np.array(sim_list)\n",
    "    print(\"Cosine Similarity size : {}\".format(len(sim_np_array)))\n",
    "    K = 25\n",
    "    output_array = sorted(list(chain.from_iterable(sim_np_array)), reverse=True)[0:K]\n",
    "    print(\"Top {} items : {}\".format(K, output_array))\n",
    "\n",
    "\n",
    "    top_k_id_hash_list = []\n",
    "    print(\"Sorted Dictionary Size : {}\".format(len(sorted_d)))\n",
    "    sorted_d_n_items = dict(itertools.islice(sorted_d.items(), K))\n",
    "    print(\"Top K Dictionary Size : {}\".format(len(sorted_d_n_items)))\n",
    "    for key in sorted_d_n_items.keys():\n",
    "        key = key[1:-1]\n",
    "        key = key.replace(\" \", \"\")\n",
    "        id_id = spark.sql('SELECT _c0 FROM parquetFile WHERE _c2 =\"' + str(key) + '\"')\n",
    "        for item in [x[\"_c0\"] for x in id_id.rdd.collect()]:\n",
    "            top_k_id_hash_list.append(item)\n",
    "    print(\"Top {} Image ID : {}\".format(K, top_k_id_hash_list))\n",
    "    top_k_id_hash_list\n",
    "    \n",
    "    score_recall = recall_score(top_k_id_hash_list, top_k_id_list, average='micro')\n",
    "    recall_value_list.append(score_recall)\n",
    "    print(\"RECALL VALUE : {}\".format(score_recall))\n",
    "print(\"RECALL LIST : {}\".format(recall_value_list))\n",
    "mean_recall = sum(recall_value_list) / len(recall_value_list)\n",
    "print(\"MEAN RECALL : {}\".format(mean_recall))\n",
    "print(\"Time : COMPLETE DATA : {}\".format(complete_bucket_time))\n",
    "print(\"Time : HASH BUCKET DATA : {}\".format(hash_bucket_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time_list = []\n",
    "for list_complete, list_hash in zip(complete_bucket_time, hash_bucket_time):\n",
    "    mean_time_list.append((list_complete + list_hash)/2)\n",
    "print(\"LIST : {}\".format(mean_time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as lib\n",
    "pred_ = IndexedRowMatrix(Pred_Factors.rdd.map(lambda x: IndexedRow(x[0],x[1]))).toBlockMatrix().transpose().toIndexedRowMatrix()\n",
    "pred_sims = pred.columnSimilarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports we'll need\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# function to generate a random Spark dense vector\n",
    "def random_dense_vector(length=10):\n",
    "    return Vectors.dense([float(np.random.random()) for i in range(length)])\n",
    "\n",
    "# create a random static dense vector\n",
    "static_vector = random_dense_vector()\n",
    "\n",
    "# create a random DF with dense vectors in column\n",
    "df = spark.createDataFrame([[random_dense_vector()] for x in range(10)], [\"myCol\"])\n",
    "df.limit(3).toPandas()\n",
    "\n",
    "# write our UDF for cosine similarity\n",
    "def cos_sim(a,b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "# apply the UDF to the column\n",
    "df = df.withColumn(\"coSim\", udf(cos_sim, FloatType())(col(\"myCol\"), array([lit(v) for v in static_vector])))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports we'll need\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "limit_value = \"1\"\n",
    "value = spark.sql(\"SELECT _c2 FROM parquetFile LIMIT\" + limit_value)\n",
    "print(value)\n",
    "\n",
    "value_array = value.withColumn(\"_c2\", split(col(\"_c2\"), \",\").cast(\"array<long>\"))\n",
    "print(value_array)\n",
    "\n",
    "# function to generate a random Spark dense vector\n",
    "def random_dense_vector(length=10):\n",
    "    return Vectors.dense([float(np.random.random()) for i in range(length)])\n",
    "\n",
    "# create a random static dense vector\n",
    "static_vector = random_dense_vector()\n",
    "print(static_vector)\n",
    "\n",
    "# create a random DF with dense vectors in column\n",
    "df = spark.createDataFrame([[random_dense_vector()] for x in range(10)], [\"myCol\"])\n",
    "df.limit(3).toPandas()\n",
    "\n",
    "# write our UDF for cosine similarity\n",
    "def cos_sim(a,b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "# apply the UDF to the column\n",
    "df = value.withColumn(\"coSim\", udf(cos_sim, FloatType())(col(\"_c2\"), array([lit(v) for v in value_array])))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "limit_value = \"1\"\n",
    "value = spark.sql(\"SELECT _c2 FROM parquetFile LIMIT\" + limit_value)\n",
    "print(value)\n",
    "\n",
    "value.withColumn(\n",
    "    \"_c2\",\n",
    "    split(col(\"_c2\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"_c2\")\n",
    ")\n",
    "value.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
